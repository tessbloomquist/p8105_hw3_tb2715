---
title: "p8105_hw3_tb2715"
author: "Tess"
date: "10/5/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(viridis)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

##Question 1
```{r}
library(p8105.datasets)
data("instacart") 
```

The instacart data set contains `r nrow(instacart)` rows and `r ncol(instacart)` columns. The columns consist of information collected from a grocery store on it's most popular items and what time the groceries were purchased. Notably, some of these variables are :

`order_dow`: which specifies the day of the week the order was placed
`aisle_id`: the unique aisle in which the product was found
`order_hour_of_day`: the time the order was made
`product_name`: item that was purchased
`department`: grocery department where the purchased item was located

Based on the dataset and headings, we can determine that as part of user 112108's purchase, they bought Bulgarian yogurt on the 4th day of the week, 10th hour of the day. It was 9 days since their prior order. Additionally, they had purchased this item before and it was located from the yogurt aisle in the dairy/eggs department. 

#identify number of aisles
```{r}
n_distinct(pull(instacart, aisle_id))
```

#arrange aisles by number of goods in an aisle
```{r}
instacart %>%
  group_by(aisle) %>%
  summarize(n_obs = n()) %>%
  arrange(desc(n_obs))
```

There are 134 aisles in this data set. The fresh vegetable, fresh fruit, and packaged vegetable fruit aisles have the most items ordered from them in that respective order. 

#plot of aisles with more than 10,000 items. 
```{r}
instacart %>%
  group_by(aisle) %>%
  summarize(n_obs = n()) %>% 
  filter(n_obs > 10000) %>%
  ggplot(aes(x = aisle, y = n_obs)) +
  geom_col() +
  theme(axis.text.x = element_text(angle = 90)) +
    labs(
    title = "Popularity of aisles with more than 10,000 purchases",
    x = "Aisle",
    y = "Number of purchases",
    caption = "Data from instacart dataset"
  ) + 
    scale_y_continuous(
    breaks = c(10000, 50000, 100000, 150000), 
    labels = c("10000", "50000", "100000", "150000"))

```

The fresh vegetable, fresh fruit, and packaged vegetable fruit aisles tower above the other products in the plot. Many of the aisles sold close to 10,000 items. 

#table of the most popular items “baking ingredients”, “dog food care”, and “packaged vegetables fruits”
```{r}
instacart %>%
  filter(aisle == "baking ingredients" | aisle == "dog food care" | aisle == "packaged vegetables fruits") %>%
  group_by(aisle, product_name) %>%
  summarize(n_obs = n()) %>%
  mutate(ranking = rank(-n_obs)) %>%
  filter(ranking <= 3) %>%
  arrange(aisle, ranking) %>%
  knitr::kable()
```

The most popular item in the dog food care aisle is sold 300x less than the most popular item from packaged fruits vegetables. Baby spinach is the most popular item by far of these three aisles. Small dog biscuits is the least popular item from this table. 


#table of the mean hour of the day when Pink Lady apples and Coffee are bought each day of the week
```{r}
instacart %>%
  filter(product_name == "Pink Lady Apple" | product_name == "Coffee Ice Cream") %>%
  group_by(product_name, order_dow) %>%
  summarize(mean_hour_of_day = mean(order_hour_of_day)) %>%
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour_of_day) %>%
  knitr::kable()
```

Pink Lady Apples and Coffee Ice cream are fairly comparable across the week. It is interesting to note that the largest difference in purchases is on day 3. In this dataset, the day that they are closest in popularity is day 1. 


##Question 2
```{r}
data("brfss_smart2010") 

#rename dataset and clean names/data
brfss =
  janitor::clean_names(brfss_smart2010)  %>%
  filter(topic == "Overall Health") %>% 
  filter(response == "Excellent" | response == "Very good" | response == "Good" | response == "Fair" | response == "Poor") %>%
  mutate(respone = as.factor(response),
  response = forcats::fct_relevel(response, c("Poor", "Fair", "Good", "Very good", "Excellent")))
```

#States observed at 7 or more locations
```{r}
#states with 7 or more testing locations in 2002
brfss %>% 
  filter(year == "2002") %>%
  group_by(locationabbr) %>%
  distinct(locationdesc) %>%
  summarize(total_testing_location = n()) %>%
  filter(total_testing_location >= 7) %>%
  arrange(total_testing_location) %>%
  knitr::kable(digits = 1)

#states with 7 or more testing locations in 2010
brfss %>% 
  filter(year == "2010") %>%
  group_by(locationabbr) %>%
  distinct(locationdesc) %>%
  summarize(total_testing_location = n()) %>%
  filter(total_testing_location >= 7) %>%
  arrange(total_testing_location) %>%
  knitr::kable(digits = 1)
```

There were more states tested at 7 or more locations in 2010 than were tested in 2002. Not all of the same states were tested at 7 or more locations between the 2 years. For example, CT had 7 testing locations in 2002 but fewer than this in 2010 and therefore did not make the list. 

#Excellent response data from brfss
```{r}
#create dataset for excellent data
excellent_data = 
brfss %>% 
  filter(response == "Excellent") %>%
  group_by(locationabbr, year) %>%
  mutate(mean_data_value = mean(data_value, na.rm = TRUE)) 

#create spaghetti plot of the excellent responses
excellent_data %>%    
ggplot(aes(x = year, y = mean_data_value, group = locationabbr, color = locationabbr)) + 
  geom_line() +
  labs(
    title = "The average data value from 2002-2010 for those with Excellent Overall Health in BRFSS",
    x = "Year",
    y = "Average data value",
    caption = "Data from brfss_smart2010 dataset "
  )

```

It is difficult to determine a pattern from the excellent response data. West Virginia data values fell dramatically in 2005, but otherwise the data is fairly clustered together. 


#dual plot of 2006 and mean_data_value vs response
```{r}
brfss %>%
  filter(year == "2006" | year == "2010",
         locationabbr == "NY") %>%
  ggplot(aes(x = response, y = data_value)) +
  geom_col() +
  facet_wrap(~year) + 
    labs(
    title = "The distribution of data value across Overall Health responses in 2006 and 2010 in NY",
    x = "Overall Health Response",
    y = "Data value",
    caption = "Data from brfss_smart2010 dataset "
  )
```

Those who responded "very good" to the overall health survey question have a higher data value in both 2006 and 2010. Those who responded "poor" have a much lower data value in both 2006 and 2010.

##Question 3
```{r}
accel = read_csv(file = "./data/accel_data.csv")

accel_tidy =  
  janitor::clean_names(accel) %>%
  mutate(weekend = if_else(day == "Saturday" | day == "Sunday", "weekend", "weekday")) %>%
  pivot_longer(
    activity_1:activity_1440, 
    names_to = "activity", 
    values_to = "activity_counts") %>%
  separate(activity, into = c("activity", "minute")) %>%
  select(-activity) %>%
  mutate(
    minute = as.numeric(minute), 
    day = as.factor(day), 
    day = forcats::fct_relevel(day, c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday")))
```

The accel data set contains `r nrow(accel_tidy)` rows and `r ncol(accel_tidy)` columns. The columns consist of information collected from an accelerometer on a 63 year old male, with a BMI of 25 who was admitted to CUMC with congestive heart failure. Notably, some of the variables in this dataset are:

`day_id`: the day accelerometer data was collected 
`activity`: an activity count corresponding to individual minutes of the day. 
`week`: the week accelerometer data was collected
`weekend`: differentiates information collected on a weekend vs weekday


#aggregate across a day to create total activity variable
```{r}
accel_tidy %>%
  group_by(day_id) %>%
  summarize(total_activity = sum(activity_counts)) %>%
  knitr::kable()
```

It appears that in later weeks of the study, activity levels drop to only 1 activity count per minute on Saturdays. This is true in week 4 and week 5. 


#create plot to show fluctuation over each day
```{r}
accel_tidy %>%
  group_by(day_id) %>%
  ggplot(aes(x = minute, y = activity_counts, color = day)) +
   geom_smooth() +
   theme(axis.text.x = element_text(angle = 90))  + 
    labs(
    title = "Distribution of Activity Counts Across Minutes in a Day for Each Day of the Week",
    x = "Minute of the daye",
    y = "Activity count",
    caption = "Data from accel dataset "
  )
  
```

A decline in activity level is seen for all days after the 1250 minute of the day. The activity levels remain low for the first 250 minutes of the day as the individual is likely sleeping. It does appear that the individual has the lowest activity levels observed on Saturdays than during the weekdays. 

